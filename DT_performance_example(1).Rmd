---
title: "DT_Performance_1"
author: "sid b"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:



IDS 572 -  Decision trees and classification performance evaluation

```{r}
#load the tidyverse set of libraries - for data manipulations
library(tidyverse)

#read the data, and examine summary statistics
mdData=read_csv('MortgageDefaultersDataSample.csv')

#look at the variables 
glimpse(mdData)

#get summary stats of the variables
summary(mdData)


#cleanup -- remove the Status variable (since the the dependent variable, Outcome, is derived from Status - so we shud not use Status as an independent var).
#  Also remove the State variable  - assume we do not want to use it here
 md<- mdData %>% select(-c("State", "Status"))
 # or you can remove by column number
 #    md <- mdData[, -c(11,13)] 
 #
```


Missing values 
```{r}
#can use the is.na(..) function to check for missing values
#   is.na(md) will check each value of md and return TRUE/FALSE for each value in md. 
#   So dim(is.na(md)) will show 6040 rows and 15 colums, corresponding to the rows & collumns in md,

#We can calculate the column sums - which will give uu the number of missing values in each column
colSums(is.na(md))

#or to get the names of columns in md which have missing values
colnames(md)[colSums(is.na(md))>0]

#which are the rows with missing values
which(is.na(md$LoanValuetoAppraised))


#Suppose we want to replace these missing values with 0 (zero)
#   we can use the replace_na function
md$LoanValuetoAppraised = as.double(md$LoanValuetoAppraised)
md<- md %>% replace_na(list(LoanValuetoAppraised=0))

#Check if this worked
colSums(is.na(md))

```



```{r}
#make sure that the variables are set to the correct attribute type -- factor, integer, numeric
str(md)

#Looks like we need to change the FirstHome and OUTCOME variables from 'character' to factors
md$First_home<- as.factor(md$First_home)

#Another way to change a variable is:
md <- md %>% mutate(OUTCOME=as.factor(OUTCOME))
str(md)
```



Decision trees using the rpart package

```{r}
library(rpart)

#develop a rpart decision tree model
rpDT1 <- rpart(OUTCOME ~ ., data=md, method="class")

#print the model -- text form
print(rpDT1)

```



Display/plot the tree 
```{r}
#plot(rpDT1, uniform=TRUE,  main="Decision Tree for Mortgage Defaulters")
#text(rpDT1, use.n=TRUE, all=TRUE, cex=.7)


#Nicer way to display the tree using the rpart.plot package
library(rpart.plot)

rpart.plot::prp(rpDT1, type=2, extra=1)
# more information on such plots are in "Plotting rpart trees with the rpart.plot package" (http://www.milbo.org/rpart-plot/prp.pdf)

```


Details on the DT model
```{r}
summary(rpDT1)
  # Q. how do you interpret each line above? 
  # At any node - what is 'expected loss', what does the 'complexity para'm value indicate? 
  # Do you understand what the surrogates' are, abd their role ?
  # Make sure we understand this output



#Variable importance as given by a decision tree model
rpDT1$variable.importance
   #In class, we covered how the importance values are obtained - we should have a good understanding of this.


```



Let's take a look at some details on complexity parameter, and how this can help determine the best (pruned) tree  
```{r}
#Tree size and performance for different cp (complexity parameter) values
printcp(rpDT1)
 #Cost-complexity parameter for different levels of pruning
 #    - this shows number of splits in the tree for different values of the  cp 
 #       parameter, and the cross-validation error

 #Best tree - the more parsimonious (simple) model with cross-validation error (xerror) which is within 1 standard deviation of the minimum xerror
 
#this is indicated by the horizontal line in the cp plot
plotcp(rpDT1)




#In the cptable display, you look up the CP value which will be closest to the min_xerror+xstd 
#      -- this is the best CP value, corresponding to the best pruned tree.  
#         To get the best tree, we can prune using this CP value.

#Instead of manually looking this up (which is ok for a small cptable, but can be cumbersome for larger cptables)
#    we can use R to find the best CP value

#Look at the rpDT1 object in the Data-Environment  panel on the right 
#   -- rpDT1$cptable has the cptable (values displyed by printcp(rpDT1)

mincp_i <- which.min(rpDT1$cptable[, 'xerror']) #the row (index) corresponding to the min xerror
print(mincp_i)
#The optimal xerror is the min_xError + xstd
optError <- rpDT1$cptable[mincp_i, "xerror"] + rpDT1$cptable[mincp_i, "xstd"]
print(optError)
#the row(index) of the xerror value which is closest to optError
optCP_i <- which.min(abs( rpDT1$cptable[,"xerror"] - optError))
print(optCP_i)
#finally, get the best CP value corresponding to optCP_i
optCP <- rpDT1$cptable[optCP_i, "CP"]


#Now we can prune the tree based on this best CP value
rpDT1_p <- prune(rpDT1, cp = optCP)

#view the plot of the pruned tree
plot(rpDT1_p, uniform=TRUE,  main="Decision Tree for Mortgage Defaulters")
text(rpDT1_p, use.n=TRUE, all=TRUE, cex=.7)

rpart.plot::prp(rpDT1_p, type=2, extra=1)
   # Compare with the unpruned tree -- do you notice how it has been pruned


```





Performance on the training data - resubtitution error
```{r}

#obtain the predictions from the DT
predDT1<-predict(rpDT1, md, type='class')

#confusion matrix using the table command
table(actuals=md$OUTCOME, preds=predDT1)

```





Next, split the data into training and validation sets, develop a model in the training data, and examine performance.
```{r}
#split the data into training and test(validation) sets - 70% for training, rest for validation
nr=nrow(md)
trnIndex = sample(1:nr, size = round(0.7*nr), replace=FALSE) #get a random 70%sample of row-indices
mdTrn=md[trnIndex,]   #training data with the randomly selected row-indices
mdTst = md[-trnIndex,]  #test data with the other row-indices

dim(mdTrn) 
dim(mdTst)


#develop a tree on the training data
rpDT2=rpart(OUTCOME ~ ., data=mdTrn, method="class")

#Obtain the model's predictions on the training data
predTrn=predict(rpDT2, mdTrn, type='class')
#Confusion table
table(pred = predTrn, true=mdTrn$OUTCOME)
#Accuracy
mean(predTrn==mdTrn$OUTCOME)

#Or you can combine the above two steps as:
#   table(pred=predict(rpDT2,mdTrn, type="class"), true=mdTrn$OUTCOME)
#To get the prob for the two classes, use predict(...without the type='class')


#Obtain the model's predictions on the test data
 #combining the two steps for ge
table(pred=predict(rpDT2,mdTst, type="class"), true=mdTst$OUTCOME)
predTst=predict(rpDT2,mdTst, type="class")
table(pred = predTst, true = mdTst$OUTCOME)
mean(predTst ==mdTst$OUTCOME)
#Q. What is the accuracy on the test data?

```





Now lets take a look at some tree-building parameters
```{r}
rpDT2g<-rpart(OUTCOME ~ ., data=mdTrn, parms = list(split ='gini'))
rpDT2i<-rpart(OUTCOME ~ ., data=mdTrn, parms = list(split ='information'))
print(rpDT2i)

#rpart.control can be used to set parameters like minCasesForSplit, minCasesAtLeaf, maxDepth,.....(see https://stat.ethz.ch/R-manual/R-devel/library/rpart/html/rpart.control.html)
rpDT4<-rpart(OUTCOME ~ ., data=mdTrn, parms = list(split ='gini'), control= rpart.control(minsplit=20, maxdepth=15))

rpDT4b <- rpart(OUTCOME ~ ., data=mdTrn, method="class", 
               parms = list(split = "information"),
               control = rpart.control(minsplit = 20, minbucket = 10, cp=0) )

```





Lift curve
```{r}
#get the 'scores' from applying the model to the data
predTrnProb=predict(rpDT4, mdTrn, type='prob')
head(predTrnProb)

#So the first column in predTrnProb gives the predicted prob(default) -- assume 'default' is the class of #interest. Next we sort the data based on these values, group into say, 10 groups (deciles), and calculate #cumulative response in each group.


#Create a data-frame with only the model scores and the actual class  (OUTCOME) values
trnSc <- mdTrn %>%  select("OUTCOME")   # selects the OUTCOME column into trnSc
trnSc$score<-predTrnProb[, 1]  #add a column named 'Score' with prob(default) values in the first column of predTrnProb  (note: first column has index of 1)

#take a look at trnSc
head(trnSc)

#sort by score
trnSc<-trnSc[order(trnSc$score, decreasing=TRUE),]

#we will next generate the cumulative sum of "default" OUTCOME values -- note that OUTCOME is a factor variable, so we cannot simply sum these;  
trnSc$cumDefault<-cumsum(trnSc$OUTCOME == "default")

#take a look at the first 10 row in trnSc
trnSc[1:10,]

#Plot the cumDefault values (y-axis) by numCases (x-axis)
plot( trnSc$cumDefault, type = "l", xlab='#cases', ylab='#default')
abline(0,max(trnSc$cumDefault)/4228, col="blue")  #diagonal line

```


Calculate the decile lift table.
```{r}
#Divide the data into 10 (for decile lift) equal groups
trnSc["bucket"]<- ntile(-trnSc[,"score"], 10)  
     # this creates a new column with group number for each row

#group the data by the 'buckets', and obtain summary statistics 
dLifts <- trnSc %>% group_by(bucket) %>% summarize(count=n(), numDefaults=sum(OUTCOME=="default"), 
              defRate=numDefaults/count,  cumDefRate=cumsum(numDefaults)/cumsum(count),
              lift = cumDefRate/(sum(trnSc$OUTCOME=="default")/nrow(trnSc)) ) 

#look at the table
dLifts


#you can do various plots, for example
plot(dLifts$bucket, dLifts$lift, xlab="deciles", ylab="Cumulative Decile Lift", type="l")
barplot(dLifts$numDefaults, main="numDefaults by decile", xlab="deciles")

#(there are different packages to give us the lift, etc., but it is useful to be able to do customized calculations)
```


```{r}
#Using the 'lift' package
library('lift')

plotLift(trnSc$score, trnSc$OUTCOME == "default")  
     #Note: this function looks for binary values for the dependent variable,, so we use trnSc$OUTCOME == "default"

#value of lift in the top decile
TopDecileLift(trnSc$score, trnSc$OUTCOME)

```



Using different classification thresholds
```{r}
CTHRESH=0.7

predTrnProb=predict(rpDT2, mdTrn, type='prob')

#Confusion table
predTrn = ifelse(predTrnProb[, 'default'] >= CTHRESH, 'default', 'non-default')
table( pred = predTrn, true=mdTrn$OUTCOME)
#Accuracy
mean(predTrn==mdTrn$OUTCOME)


```




ROC curves (using the ROCR package)
```{r}
library('ROCR')

#obtain the scores from the model for the class of interest, here, the prob('default')
scoreTst=predict(rpDT2, mdTst, type="prob")[,'default']  
   #same as predProbTst

#now apply the prediction function from ROCR to get a prediction object
rocPredTst = prediction(scoreTst, mdTst$OUTCOME, label.ordering = c('non-default', 'default'))  

#obtain performance using the function from ROCR, then plot
perfROCTst=performance(rocPredTst, "tpr", "fpr")
plot(perfROCTst)
abline(0,1)



#for a different model 
scoreTst_dt4=predict(rpDT4, mdTst, type="prob")[,'default']  
rocPredTst_dt4 = prediction(scoreTst_dt4, mdTst$OUTCOME, label.ordering = c('non-default', 'default')) 
perfROCTst_dt4=performance(rocPredTst_dt4, "tpr", "fpr")

plot(perfROCTst_dt4, add=TRUE, col="blue")   #adds to previous plor   #add a legend
legend('bottomright', c('dt2', 'dt4'), lty=1, col=c('black', 'blue'))

```


Other performance from ROCR
```{r}

#AUC value
aucPerf=performance(rocPredTst, "auc")
aucPerf@y.values

#Accuracy 
accPerf <-performance(rocPredTst, "acc")
plot(accPerf)

 #optimal threhold for max overall accuracy
accPerf@x.values[[1]][which.max(accPerf@y.values[[1]])]


#optimal cost with different costs for fp and fn
costPerf = performance(rocPredTst, "cost", cost.fp = 1, cost.fn = 3)
costPerf@x.values[[1]][which.min(costPerf@y.values[[1]])]


#Lift curve
liftPerf <-performance(rocPredTst, "lift", "rpp")
plot(liftPerf, main="Lift chart")

#Q. are these plots similar to what we obtained earlier from our own calculations

```



C5.0 decision trees
```{r}
library(C50)

#build a tree model
c5_DT1 <- C5.0(OUTCOME ~ ., data=mdTrn, control=C5.0Control(minCases=10))

#model details
summary(c5_DT1)




#Rules - DT simplified to a set of rules
c5_rules1 <- C5.0(OUTCOME ~ ., data=mdTrn, control=C5.0Control(minCases=10), rules=TRUE)
summary(c5_rules1)

#Performance - training
predTrnProb_c5dt1 <- predict(c5_DT1, mdTrn, type='prob')
predTrn = ifelse(predTrnProb_c5dt1[, 'default'] >= 0.5, 'default', 'non-default')
table( pred = predTrn, true=mdTrn$OUTCOME)
#Accuracy
mean(predTrn==mdTrn$OUTCOME)

#Performance  - test
predTstProb_c5dt1 <- predict(c5_DT1, mdTst, type='prob')
predTst = ifelse(predTstProb_c5dt1[, 'default'] >= 0.5, 'default', 'non-default')
table( pred = predTst, true=mdTst$OUTCOME)
#Accuracy
mean(predTst==mdTst$OUTCOME)


##Performance of rules - test
predTstProb_c5dt1 <- predict(c5_rules1, mdTst, type='prob')
predTst = ifelse(predTstProb_c5dt1[, 'default'] >= 0.5, 'default', 'non-default')
table( pred = predTst, true=mdTst$OUTCOME)
#Accuracy
mean(predTst==mdTst$OUTCOME)

#Question: notice any performance differences between rpart tree, c50 tree and c50Rules ?  
#    Which model would you prefer ?
#   You can do a ROC plot and get AUC for the DT models.


```



