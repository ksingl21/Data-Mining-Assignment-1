---
  title: "Assignment 1 Classification models - bankData"
author: "Athena Gonzalez, Rosaura Ocampo, Kapil Singla"
---

1a. Data Exploration
```{r} 

#load the tidyverse set of libraries - for data manipulations
library(tidyverse)

#read the data, and examine summary statistics
bankData=read_csv2('bank-full.csv')

#look at the variables 
glimpse(bankData)

#get summary stats of the variables
summary(bankData)

#Convert the chr variables to factor
bData <- bankData %>% mutate_if(is.character,as.factor)

str(bData)

#calculate the column sums - the number of missing values in each column
colSums(is.na(bData))
#there are no missing values

#get summary statistics on the variables
summary(bData)
```


1b. Data Exploration
```{r}
#Number of yes,no values in the dependent variable y
bData %>% group_by(y) %>% summarize(n())
#Proportion of yes,no values in the dependent variable y                                  
bData %>% group_by(y) %>% summarize(n=n()) %>% mutate(proportion=n/sum(n))


#summarize all numeric variables, grouped by dependent(target) variable
bData %>% group_by(y) %>% summarize_if(is.numeric, mean)
#Separated data in each numeric variable by response variable and found the mean

#summarize the factor variables
bData %>% group_by(job, y) %>% summarize( n=n())
bData %>% group_by(marital, y) %>% summarize( n=n())
bData %>% group_by(education, y) %>% summarize( n=n())
#for each type of factor variable, gives the count of yes,no values of y

#Proportions of the factor variables
bData %>% group_by(job, y) %>% summarize( n=n()) %>% mutate(proportion=n/sum(n))
bData %>% group_by(marital, y) %>% summarize( n=n()) %>% mutate(proportion=n/sum(n))
bData %>% group_by(education, y) %>% summarize( n=n()) %>% mutate(proportion=n/sum(n))


#Look at other variables
bData %>% group_by(poutcome, y) %>% tally()
bData %>% group_by(contact, y) %>% tally()
bData %>% group_by(campaign, y) %>% tally()

```


1c. Data Exploration
```{r}
#Look at the age variable 
boxplot(bData$age)
#boxplot
ggplot(bData, aes(age,  color=y) ) + geom_boxplot()
#density plot
ggplot(bData, aes(age,  color=y) ) + geom_density()

#view response by different age ranges 
bData$ageGroup <- cut(bData$age, breaks = c(0, 30, 40, 50, 60, 100))
bData %>% group_by(ageRanges, y) %>% tally()
bData %>% group_by(ageRanges, y) %>% tally() %>% mutate(propResp=n/sum(n))

#plot the response rate by age ranges
tmp <-bData %>% group_by(ageRanges, y) %>% tally() %>% mutate(propResp=n/sum(n)) 
ggplot(tmp, aes(y=propResp, x=ageRanges, fill=y))+geom_bar(stat = 'identity')


#Look at duration of calls 
summary(bData$duration)
ggplot(bData, aes(duration,  color=y) ) + geom_boxplot()

#Look at number of calls
summary(bData$campaign)
ggplot(bData, aes(campaign,  color=y) ) + geom_boxplot()

#examine duration and number of calls relationship, and by response(y=yes/no)
ggplot(bData, aes(duration, campaign, color=y))+geom_point()

```


Selecting Data for Developing Predictive Models
```{r}
#select variables to be used for developing predictive modes - only using client variables so the following are removed
mData <- bData %>% select(-c('contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome'))

#Removing ageRanges which was created for data exploration
mData <- mData %>% select(-c('ageRanges'))
```


```{r}
library(rpart)

#develop a rpart decision tree model
rpDT1 <- rpart(y ~ ., data=mData, method="class")

#print the model -- text form
print(rpDT1)
   #what does the model look like?? It does not have any branches.

#to correct for class-imbalance, use the prior parameter
rpDT2 = rpart(y ~ ., data=mData, method="class", parms=list(prior=c(.5,.5)))

#Display/Plot
plot(rpDT2, uniform=TRUE,  main="Decision Tree for Bank marketing response")
text(rpDT2, use.n=TRUE, all=TRUE, cex=.7)


#Nicer way to display the tree using the rpart.plot package
library(rpart.plot)
rpart.plot::prp(rpDT2, type=2, extra=1) #Tree is created with branches

#Details on DT 
summary(rpDT2)

#Variable importance as given by a decision tree model
rpDT1$variable.importance

```
Complexity Parameter Details
```{r}
#Grow a tree, with cp=0
rpDT1 = rpart(y ~ ., data=mData, method="class", control = rpart.control(cp = 0.0), parms=list(prior=c(.5,.5)))
#Is this a larger tree? It is a large tree.

#Tree size and performance for different cp (complexity parameter) values
printcp(rpDT1)
 #Cost-complexity parameter for different levels of pruning
 #    - this shows number of splits in the tree for different values of the  cp 
 #       parameter, and the cross-validation error

#Best tree - the more parsimonious (simple) model with cross-validation error (xerror) which is within 1 standard deviation of the minimum xerror
#this is indicated by the horizontal line in the cp plot
plotcp(rpDT1)


#In the cptable display, you look up the CP value which will be closest to the min_xerror+xstd 
#      -- this is the best CP value, corresponding to the best pruned tree.  
#         To get the best tree, we can prune using this CP value.

mincp_i <- which.min(rpDT1$cptable[, 'xerror'])  #the row (index) corresponding to the min xerror

#The optimal xerror is the min_xError + xstd
optError <- rpDT1$cptable[mincp_i, "xerror"] + rpDT1$cptable[mincp_i, "xstd"]

#the row(index) of the xerror value which is closest to optError
optCP_i <- which.min(abs( rpDT1$cptable[,"xerror"] - optError))

#finally, get the best CP value corresponding to optCP_i
optCP <- rpDT1$cptable[optCP_i, "CP"]


#Now we can prune the tree based on this best CP value
rpDT1_p <- prune(rpDT1, cp = optCP)

#view the plot of the pruned tree
plot(rpDT1_p, uniform=TRUE,  main="Decision Tree for Bank Marketing")
text(rpDT1_p, use.n=TRUE, all=TRUE, cex=.7)

   # Compare with the unpruned tree -- do you notice how it has been pruned It is still a large tree after prunning. 

```
Performance on the training data
```{r}
#obtain the predictions from the DT
predDT1<-predict(rpDT1_p, mData, type='class')

#confusion matrix using the table command
table(actuals=bData$y, preds=predDT1)
```


Training and Validation Sets
```{r}
#Splitting the data into training and test(validation) sets - 70% for training, 30% for validation
nr=nrow(mData)
trnIndex = sample(1:nr, size = round(0.7*nr), replace=FALSE) #randomly choosing 70% of row-indices
mdTrn=mData[trnIndex,]   #training data with the randomly selected row-indices
mdTst = mData[-trnIndex,]  #test data with the other row-indices

dim(mdTrn) 
dim(mdTst)
```


2a. Develop rpart Decision Tree Model
```{r}
#develop a tree on the training data
rpDT2=rpart(y ~ ., data=mdTrn, method="class",  control = rpart.control(cp = 0.0), parms=list(prior=c(.5,.5)) )


#Obtain the model's predictions on the training data
predTrn=predict(rpDT2, mdTrn, type='class')
#Confusion table
table(pred = predTrn, true=mdTrn$y)
#Accuracy
mean(predTrn==mdTrn$y)

#Obtain the model's predictions on the test data
predTst=predict(rpDT2, mdTst, type='class')
#Confusion table
table(pred = predTst, true=mdTst$y)
#Accuracy
mean(predTst==mdTst$y)


#As in code above, look at the cptable, find the optimal cp value and prune using the best cp value

mincp_i <- which.min(rpDT2$cptable[, 'xerror'])  #the row (index) corresponding to the min xerror

#The optimal xerror is the min_xError + xstd
optError <- rpDT2$cptable[mincp_i, "xerror"] + rpDT2$cptable[mincp_i, "xstd"]

#the row(index) of the xerror value which is closest to optError
optCP_i <- which.min(abs( rpDT2$cptable[,"xerror"] - optError))

#finally, get the best CP value corresponding to optCP_i
optCP <- rpDT2$cptable[optCP_i, "CP"]

#Now we can prune the tree based on this best CP value
rpDT2_p <- prune(rpDT2, cp = optCP)


#What is the classification performance of the pruned tree on training and test data,
#   and how does this compare with performance of the unpruned tree

#Obtain the model's predictions on the training data
predTrn=predict(rpDT2_p, mdTrn, type='class')
#Confusion table
table(pred = predTrn, true=mdTrn$y)
#Accuracy
mean(predTrn==mdTrn$y)

#Obtain the model's predictions on the test data
predTst=predict(rpDT2_p, mdTst, type='class')
#Confusion table
table(pred = predTst, true=mdTst$y)
#Accuracy
mean(predTst==mdTst$y)

```



ROC curves (using the ROCR package)
```{r}
library('ROCR')

#obtain the scores from the model for the class of interest, here, the prob('default')
scoreTst=predict(rpDT2_p, mdTst, type="prob")[,'yes']  
   #same as predProbTst

#now apply the prediction function from ROCR to get a prediction object
rocPredTst = prediction(scoreTst, mdTst$y, label.ordering = c('no', 'yes'))  

#obtain performance using the function from ROCR, then plot
perfROCTst=performance(rocPredTst, "tpr", "fpr")
plot(perfROCTst)
abline(0,1)


#How does this compare with the training data?


#AUC value
aucPerf=performance(rocPredTst, "auc")
aucPerf@y.values

#Accuracy 
accPerf <-performance(rocPredTst, "acc")
plot(accPerf)

 #optimal threshold for max overall accuracy
accPerf@x.values[[1]][which.max(accPerf@y.values[[1]])]


#optimal cost with different costs for fp and fn
costPerf = performance(rocPredTst, "cost", cost.fp = 1, cost.fn = 3)
costPerf@x.values[[1]][which.min(costPerf@y.values[[1]])]




#Q. are these plots similar to what we obtained earlier from our own calculations

```


2b. Develop C50 Decision Tree and Rules
```{r}
#Develop C5.0 Model
library(C50)

#build a tree model
c5DT1 <- C5.0(y ~ ., data=mdTrn, control=C5.0Control(minCases=10))

#performance without Cost parameter in the model.
 #performance on Training Data
predTrn <- predict(c5DT1, mdTrn)
table( pred = predTrn, true=mdTrn$y)
mean(predTrn==mdTrn$y)

#performance on Test Data
predTst <- predict(c5DT1, mdTst)
table( pred = predTst, true=mdTst$y)
mean(predTst==mdTst$y)

#variable importance
C5imp(c5DT1)

#Can try to use costs to try overcome class imbalance in data
costMatrix <- matrix(c(
    0,   1,
    10,  0),
   2, 2, byrow=TRUE)
rownames(costMatrix) <- colnames(costMatrix) <- c("yes", "no")

#Adding Cost parameter to model to calculate performance.
c5DT1 <- C5.0(y ~ ., data=mdTrn, control=C5.0Control(minCases=10), costs=costMatrix)

#performance on Training Data
predTrn <- predict(c5DT1, mdTrn)
table( pred = predTrn, true=mdTrn$y)
mean(predTrn==mdTrn$y)

#performance on Test Data
predTst <- predict(c5DT1, mdTst)
table( pred = predTst, true=mdTst$y)
mean(predTst==mdTst$y)


#variable importance
C5imp(c5DT1)


```


2c. Develop a Random Forest Model
```{r}
library('randomForest')
set.seed(576)

rfModel = randomForest(y ~ ., data=mdTrn, ntree=200, importance=TRUE )
importance(rfModel) %>% view()
varImpPlot(rfModel)

#Accuracy-
accPerf <-performance(rocPredTst, "acc")
 plot(accPerf)

#AUC-
aucPerf=performance(rocPredTst, "auc")
aucPerf@y.values


rfModel = randomForest(y ~ ., data=mdTrn, ntree=100, importance=TRUE )
importance(rfModel) %>% view()
varImpPlot(rfModel)

#Accuracy-
accPerf <-performance(rocPredTst, "acc")
 plot(accPerf)

#AUC-
aucPerf=performance(rocPredTst, "auc")
aucPerf@y.values


rfModel = randomForest(y ~ ., data=mdTrn, ntree=50, importance=TRUE )
importance(rfModel) %>% view()
varImpPlot(rfModel)

#Accuracy-
accPerf <-performance(rocPredTst, "acc")
 plot(accPerf)

#AUC-
aucPerf=performance(rocPredTst, "auc")
aucPerf@y.values



#Classification performance
CTHRESH = 0.5

#Training Data
rfPred<-predict(rfModel,mdTrn, type="prob")
pred = ifelse(rfPred[, 'yes'] >= CTHRESH, 'yes', 'no')
table( pred = pred, true=mdTrn$y)
mean(pred==mdTrn$y)

rfPred<-predict(rfModel,mdTst, type="prob")
pred = ifelse(rfPred[, 'yes'] >= CTHRESH, 'yes', 'no')
table( pred = pred, true=mdTst$y)
mean(pred==mdTst$y)


#Roc Curve
perf_rfTst=performance(prediction(predict(rfModel,mdTst, type="prob")[,2], mdTst$y), "tpr", "fpr")
plot(perf_rfTst)
```


##Lift Analysis
predTrnProb=predict(rfModel, mdTrn, type='prob')
head(predTrnProb)

trnSc <- mdTrn %>%  select("y")   
trnSc$score<-predTrnProb[, 2]

trnSc<-trnSc[order(trnSc$score, decreasing=TRUE),]
trnSc$cumResponse<-cumsum(trnSc$y == "yes")
trnSc[1:10,]
plot( trnSc$cumResponse, type = "l", xlab='#cases', ylab='#default')
abline(0,max(trnSc$cumResponse)/nrow(trnSc), col="blue")

trnSc["y"]<- ntile(-trnSc[,"score"], 10) 

dLifts <- trnSc %>% group_by(numResponse) %>% summarize(count=n(), numResponse=sum(y=="yes"), 
respRate=numResponse/count,  cumRespRate=cumsum(numResponse)/cumsum(count), lift = cumRespRate/(sum(trnSc$y=="yes")/nrow(trnSc)) ) 



summary(dLifts)

plot(dLifts$numResponse, dLifts$lift, xlab="deciles", ylab="Cumulative Decile Lift", type="l")
barplot(dLifts$numResponse, main="numDefaults by decile", xlab="deciles")



2d. Develop a GBM Model
```{r}
library(gbm)

#gbm looks for 0,1 values in the dependent variable -- obtained here using unclass()
gbm_M1 <- gbm(formula=unclass(y)-1 ~., data=mdTrn,distribution = "bernoulli", n.trees=1000, shrinkage=0.025, interaction.depth = 4, bag.fraction=0.5, cv.folds = 5,  n.cores=NULL)  

gbm_M1 <- gbm(formula=unclass(y)-1 ~., data=mdTrn,distribution = "bernoulli", n.trees=500, shrinkage=0.025, interaction.depth = 4, bag.fraction=0.5, cv.folds = 5,  n.cores=NULL)  

gbm_M1 <- gbm(formula=unclass(y)-1 ~., data=mdTrn,distribution = "bernoulli", n.trees=1000, shrinkage=0.125, interaction.depth = 4, bag.fraction=0.5, cv.folds = 5,  n.cores=NULL)  


#Test Data, kept at n=1000 and shrinkage is 0.025
gbm_M1 <- gbm(formula=unclass(y)-1 ~., data=mdTst,distribution = "bernoulli", n.trees=1000, shrinkage=0.025, interaction.depth = 4, bag.fraction=0.5, cv.folds = 5,  n.cores=NULL)  


#Look at the resulting model
summary(gbm_M1)

#variable importance
summary(gbm_M1)
#variables of importance include age, balance, job, housing, education, loan, marital, and default. All listed in decreasing order ot importance and rel. inf values.

#plot of cv performance by iterations
bestIter<-gbm.perf(gbm_M1, method='cv')
  
   #Best Iteration according to plot was a vertical dashed blue line right before the number 800 on the x-axis (n=1000, sh=0.025)
   #400 when n=500, sh=0.025

scores_gbmM1<- predict(gbm_M1, newdata = mdTrn, n.tree= bestIter, type="response")
head(scores_gbmM1)
     
     #Lift Analysis ntree=1000, shrinkage 0.025
predTrnProb=predict(gbm_M1, mdTrn, type='prob')
head(predTrnProb)

trnSc <- mdTrn %>%  select("y")   
trnSc$score<-predTrnProb[, 2]

trnSc<-trnSc[order(trnSc$score, decreasing=TRUE),]
trnSc$cumResponse<-cumsum(trnSc$y == "yes")
trnSc[1:10,]
plot( trnSc$cumResponse, type = "l", xlab='#cases', ylab='#default')
abline(0,max(trnSc$cumResponse)/nrow(trnSc), col="blue")

trnSc["bucket"]<- ntile(-trnSc[,"score"], 10) 

dLifts <- trnSc %>% group_by(bucket) %>% summarize(count=n(), numResponse=sum(y=="yes"), 
respRate=numResponse/count,  cumRespRate=cumsum(numResponse)/cumsum(count), lift = cumRespRate/(sum(trnSc$y=="yes")/nrow(trnSc)) ) 
summary(dLifts)

plot(dLifts$numResponse, dLifts$lift, xlab="deciles", ylab="Cumulative Decile Lift", type="l")
barplot(dLifts$numResponse, main="numDefaults by decile", xlab="deciles")



library('ROCR')
#ROC curve
pred_gbmM1 <- prediction( scores_gbmM1, mdTrn$y, label.ordering = c("no", "yes"))
rocPerf_gbmM1 <-performance(pred_gbmM1, "tpr","fpr")
plot(rocPerf_gbmM1)
abline(a=0, b= 1)

#comparing to test data
 gbm_M1 <- gbm(formula=unclass(y)-1 ~., data=mdTst,distribution = "bernoulli", n.trees=1000, shrinkage=0.025, interaction.depth = 4, bag.fraction=0.5, cv.folds = 5,  n.cores=NULL)  

summary(gbm_M1)
 bestIter<-gbm.perf(gbm_M1, method='cv')
scores_gbmM1<- predict(gbm_M1, newdata = mdTst, n.tree= bestIter, type="response")

head(scores_gbmM1)
#[1] 0.06513680 0.04140867 0.08542534 0.04872887 0.18309469 0.11230618

pred_gbmM1 <- prediction( scores_gbmM1, mdTst$y, label.ordering = c("no", "yes"))
rocPerf_gbmM1 <-performance(pred_gbmM1, "tpr","fpr")
plot(rocPerf_gbmM1)
abline(a=0, b= 1)
```

aucPerf_gbmM1=performance(pred, "auc")
aucPerf_gbmM1@y.values


2e. Develop a Naive Bayes Model
```{r}
library(naivebayes)

#Develop a naive bayes model
nbM1<-naive_bayes(y ~ ., data = mdTrn)
plot(nbM1)
summary(nbM1)

nbPred1Trn = predict(nbM1, mdTrn, type='prob')

#ROC curve for nbM1 trn
pred_nbM1Trn = prediction(nbPred1Trn[,2], mdTrn$y)
perf_nbM1Trn <- performance(pred_nbM1Trn, "tpr","fpr")
plot(perf_nbM1Trn)
abline(a=0, b= 1)

#AUC
aucPerfnbM1 <- performance(pred_nbM1Trn , "auc")
aucPerf@y.values

#Accuracy 
accPerfnbM1 <- performance(pred_nbM1Trn, "acc")
plot(accPerf)

#optimal threshold for max overall accuracy
accPerfnbM1@x.values[[1]][which.max(accPerfnbM1@y.values[[1]])]


#optimal cost with different costs for fp and fn
costPerfnbM1 = performance(pred_nbM1Trn, "cost", cost.fp = 1, cost.fn = 3)
costPerfnbM1@x.values[[1]][which.min(costPerfnbM1@y.values[[1]])]

#Lift Curve for nbM1 trn
sc_nbM1Trn <- mdTrn %>%  select("y")
sc_nbM1Trn$score <- nbPred1Trn[, 2]
sc_nbM1Trn <- sc_nbM1Trn[order(sc_nbM1Trn$score, decreasing=TRUE),]
sc_nbM1Trn$cumResponse<-cumsum(sc_nbM1Trn$y == "yes")
plot(sc_nbM1Trn$cumResponse, type = "l", xlab='#cases', ylab='#default')
abline(0,max(sc_nbM1Trn$cumResponse)/nrow(sc_nbM1Trn), col="blue")  #diagonal line

nbPred1Tst = predict(nbM1, mdTst, type='prob')

#Lift Curve for nbM1 tst
predTstProb=predict(nbM1, mdTst, type='prob')
sc_nbM1Tst <- mdTst %>%  select("y")
sc_nbM1Tst$score <- nbPred1Tst[, 2]
sc_nbM1Tst<-sc_nbM1Tst[order(sc_nbM1Tst$score, decreasing=TRUE),]
sc_nbM1Tst$cumResponse<-cumsum(sc_nbM1Tst$y == "yes")

plot(sc_nbM1Tst$cumResponse, type = "l", xlab='#cases', ylab='#default')
abline(0,max(sc_nbM1Tst$cumResponse)/nrow(tstSc), col="blue")  #diagonal line



#Develop a naive-Bayes model with kernel density estimation
nbM2<-naive_bayes(y ~ ., data = mdTrn, usekernel = T) 
plot(nbM2)

#ROC curve for nbM2
nbPred2 = predict(nbM2, mdTrn, type='prob')
pred_nbM2=prediction(nbPred2[,2], mdTrn$y)
rocPerf_nbM2 <- performance(pred_nbM2, "tpr","fpr")
plot(rocPerf_nbM2)
abline(a=0, b= 1)

#attempting various thresholds
THRESH1=0.2
table(pred=nbPred1Trn[, 2] > THRESH1, actual=mdTrn$y)
THRESH2=0.5
table(pred=nbPred1Trn[, 2] > THRESH2, actual=mdTrn$y)
THRESH3=0.8
table(pred=nbPred1Trn[, 2] > THRESH3, actual=mdTrn$y)

```


2f. Compare Performance of Different Models Developed
```{r}
#rpart DT model
dtrPred <- predict(rpDT2_p, mdTst, type="prob")[,'yes']  
dtrROCPred <- prediction( dtrPred, mdTst$y, label.ordering = c("no", "yes") )
dtrPerfROC <- performance(dtrROCPred, "tpr", "fpr")
plot(dtrPerfROC, add=TRUE, col='blue') 

#random forest model
rfPred <-predict(rfModel,mdTst, type="prob")[, 'yes']
rfROCPred <- prediction( rfPred, mdTst$y, label.ordering = c("no", "yes") )
rfPerfROC <- performance(rfROCPred, "tpr", "fpr")
plot(rfPerfROC, add=TRUE, col='green') 

#gbm model
gbmPred <- predict(gbm_M1, newdata = mdTst, n.tree= bestIter, type="response")
gbmROCPred <- prediction( gbmPred, mdTst$y, label.ordering = c("no", "yes") )
gbmPerfROC <- performance(gbmROCPred, "tpr", "fpr")
plot(gbmPerfROC, add=TRUE, col='red')

#naive-Bayes model
nbPred <- predict(nbM1, mdTst, type='prob')[, "yes"]
nbROCPred <- prediction( nbPred, mdTst$y, label.ordering = c("no", "yes") )
nbPerfROC <- performance(nbROCPred, "tpr", "fpr")
plot(nbPerfROC, col='black') 

#Add legend
legend('bottomright', c('nB', 'rpartDT', 'rf', 'gbm'), lty=1, col=c('black', 'blue', 'green', 'red'))
abline(0,1)  #add the diagonal reference line



#What do ROC curves on the training data look like ?
# Do some models exhibit greater overfit to the training data?

```

