---
  title: "Assignment 1 Classification models - bankData"
author: "Athena Gonzalez, Rosaura Ocampo, Kapil Singla"
---

1a. Data Exploration
```{r} 

#load the tidyverse set of libraries - for data manipulations
library(tidyverse)

#read the data, and examine summary statistics
bankData=read_csv2('bank-full.csv')

#look at the variables 
glimpse(bankData)

#get summary stats of the variables
summary(bankData)

#Convert the chr variables to factor
bData <- bankData %>% mutate_if(is.character,as.factor)

str(bData)

#calculate the column sums - the number of missing values in each column
colSums(is.na(bData))
#there are no missing values

#get summary statistics on the variables
summary(bData)
```


1b. Data Exploration
```{r}
#Number of yes,no values in the dependent variable y
bData %>% group_by(y) %>% summarize(n())
#Proportion of yes,no values in the dependent variable y                                  
bData %>% group_by(y) %>% summarize(n=n()) %>% mutate(proportion=n/sum(n))


#summarize all numeric variables, grouped by dependent(target) variable
bData %>% group_by(y) %>% summarize_if(is.numeric, mean)
#Separated data in each numeric variable by response variable and found the mean

#summarize the factor variables
bData %>% group_by(job, y) %>% summarize( n=n())
bData %>% group_by(marital, y) %>% summarize( n=n())
bData %>% group_by(education, y) %>% summarize( n=n())
#for each type of factor variable, gives the count of yes,no values of y

#Proportions of the factor variables
bData %>% group_by(job, y) %>% summarize( n=n()) %>% mutate(proportion=n/sum(n))
bData %>% group_by(marital, y) %>% summarize( n=n()) %>% mutate(proportion=n/sum(n))
bData %>% group_by(education, y) %>% summarize( n=n()) %>% mutate(proportion=n/sum(n))


#Look at other variables
bData %>% group_by(poutcome, y) %>% tally()
bData %>% group_by(contact, y) %>% tally()
bData %>% group_by(campaign, y) %>% tally()

```


1c. Data Exploration
```{r}
#Look at the age variable 
boxplot(bData$age)
#boxplot
ggplot(bData, aes(age,  color=y) ) + geom_boxplot()
#density plot
ggplot(bData, aes(age,  color=y) ) + geom_density()

#view response by different age ranges 
bData$ageRanges <- cut(bData$age, breaks = c(0, 30, 40, 50, 60, 100))
bData %>% group_by(ageRanges, y) %>% tally()
bData %>% group_by(ageRanges, y) %>% tally() %>% mutate(propResp=n/sum(n))

#plot the response rate by age ranges
tmp <-bData %>% group_by(ageRanges, y) %>% tally() %>% mutate(propResp=n/sum(n)) 
ggplot(tmp, aes(y=propResp, x=ageRanges, fill=y))+geom_bar(stat = 'identity')


#Look at duration of calls 
summary(bData$duration)
ggplot(bData, aes(duration,  color=y) ) + geom_boxplot()

#Look at number of calls
summary(bData$campaign)
ggplot(bData, aes(campaign,  color=y) ) + geom_boxplot()

#examine duration and number of calls relationship, and by response(y=yes/no)
ggplot(bData, aes(duration, campaign, color=y))+geom_point()

```


Selecting Data for Developing Predictive Models
```{r}
#select variables to be used for developing predictive modes - only using client variables so the following are removed
mData <- bData %>% select(-c('contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome'))

#Removing ageRanges which was created for data exploration
mData <- mData %>% select(-c('ageRanges'))
```


```{r}



Training and Validation Sets
```{r}
#Splitting the data into training and test(validation) sets - 70% for training, 30% for validation
nr=nrow(mData)
trnIndex = sample(1:nr, size = round(0.7*nr), replace=FALSE) #randomly choosing 70% of row-indices
mdTrn=mData[trnIndex,]   #training data with the randomly selected row-indices
mdTst = mData[-trnIndex,]  #test data with the other row-indices

dim(mdTrn) 
dim(mdTst)
```


2a. Develop rpart Decision Tree Model
```{r}
library(rpart)

#develop a rpart decision tree model
rpDT1 <- rpart(y ~ ., data=mData, method="class")

#print the model -- text form
print(rpDT1)
   #what does the model look like?? It does not have any branches.

#to correct for class-imbalance, use the prior parameter
rpDT2 = rpart(y ~ ., data=mData, method="class", parms=list(prior=c(.5,.5)))

#Display/Plot
plot(rpDT2, uniform=TRUE,  main="Decision Tree for Bank marketing response")
text(rpDT2, use.n=TRUE, all=TRUE, cex=.7)


#Nicer way to display the tree using the rpart.plot package
library(rpart.plot)
rpart.plot::prp(rpDT2, type=2, extra=1) #Tree is created with branches

#Details on DT 
summary(rpDT2)

#Variable importance as given by a decision tree model
rpDT1$variable.importance

```
Complexity Parameter Details
```{r}
#Grow a tree, with cp=0
rpDT1 = rpart(y ~ ., data=mData, method="class", control = rpart.control(cp = 0.0), parms=list(prior=c(.5,.5)))
#Is this a larger tree? It is a large tree.

#Tree size and performance for different cp (complexity parameter) values
printcp(rpDT1)
 

plotcp(rpDT1)


#In the cptable display, you look up the CP value which will be closest to the min_xerror+xstd 
#      -- this is the best CP value, corresponding to the best pruned tree.  
#         To get the best tree, we can prune using this CP value.

mincp_i <- which.min(rpDT1$cptable[, 'xerror'])  #the row (index) corresponding to the min xerror

#The optimal xerror is the min_xError + xstd
optError <- rpDT1$cptable[mincp_i, "xerror"] + rpDT1$cptable[mincp_i, "xstd"]

#the row(index) of the xerror value which is closest to optError
optCP_i <- which.min(abs( rpDT1$cptable[,"xerror"] - optError))

#finally, get the best CP value corresponding to optCP_i
optCP <- rpDT1$cptable[optCP_i, "CP"]


#Now we can prune the tree based on this best CP value
rpDT1_p <- prune(rpDT1, cp = optCP)

#view the plot of the pruned tree
plot(rpDT1_p, uniform=TRUE,  main="Decision Tree for Bank Marketing")
text(rpDT1_p, use.n=TRUE, all=TRUE, cex=.7)

   # Compare with the unpruned tree -- do you notice how it has been pruned. It is still a large tree after prunning. 

```
Performance on the training data
```{r}
#obtain the predictions from the DT
predDT1<-predict(rpDT1_p, mData, type='class')

#confusion matrix using the table command
table(actuals=bData$y, preds=predDT1)
```

#develop a tree on the training data
rpDT2=rpart(y ~ ., data=mdTrn, method="class",  control = rpart.control(cp = 0.0), parms=list(prior=c(.5,.5)) )


#Obtain the model's predictions on the training data
predTrn=predict(rpDT2, mdTrn, type='class')
#Confusion table
table(pred = predTrn, true=mdTrn$y)
#Accuracy
mean(predTrn==mdTrn$y)

#Obtain the model's predictions on the test data
predTst=predict(rpDT2, mdTst, type='class')
#Confusion table
table(pred = predTst, true=mdTst$y)
#Accuracy
mean(predTst==mdTst$y)


#As in code above, look at the cptable, find the optimal cp value and prune using the best cp value

mincp_i <- which.min(rpDT2$cptable[, 'xerror'])  #the row (index) corresponding to the min xerror

#The optimal xerror is the min_xError + xstd
optError <- rpDT2$cptable[mincp_i, "xerror"] + rpDT2$cptable[mincp_i, "xstd"]

#the row(index) of the xerror value which is closest to optError
optCP_i <- which.min(abs( rpDT2$cptable[,"xerror"] - optError))

#finally, get the best CP value corresponding to optCP_i
optCP <- rpDT2$cptable[optCP_i, "CP"]

#Now we can prune the tree based on this best CP value
rpDT2_p <- prune(rpDT2, cp = optCP)


#What is the classification performance of the pruned tree on training and test data,
#   and how does this compare with performance of the unpruned tree

#Obtain the model's predictions on the training data
predTrn=predict(rpDT2_p, mdTrn, type='class')
#Confusion table
table(pred = predTrn, true=mdTrn$y)
#Accuracy
mean(predTrn==mdTrn$y)

#Obtain the model's predictions on the test data
predTst=predict(rpDT2_p, mdTst, type='class')
#Confusion table
table(pred = predTst, true=mdTst$y)
#Accuracy
mean(predTst==mdTst$y)

```


2b. Develop C50 Decision Tree and Rules
```{r}
#Develop C5.0 Model
library(C50)

#build a tree model
c5DT1 <- C5.0(y ~ ., data=mdTrn, control=C5.0Control(minCases=10))

#performance without Cost parameter in the model.
 #performance on Training Data
predTrn <- predict(c5DT1, mdTrn)
table( pred = predTrn, true=mdTrn$y)
mean(predTrn==mdTrn$y)

#performance on Test Data
predTst <- predict(c5DT1, mdTst)
table( pred = predTst, true=mdTst$y)
mean(predTst==mdTst$y)

#Can try to use costs to try overcome class imbalance in data
costMatrix <- matrix(c(
    0,   1,
    10,  0),
   2, 2, byrow=TRUE)
rownames(costMatrix) <- colnames(costMatrix) <- c("yes", "no")

#Adding Cost parameter to model to calculate performance.
c5DT1_c <- C5.0(y ~ ., data=mdTrn, control=C5.0Control(minCases=10), costs=costMatrix)

#performance on Training Data
predTrn <- predict(c5DT1_c, mdTrn)
table( pred = predTrn, true=mdTrn$y)
mean(predTrn==mdTrn$y)

#performance on Test Data
predTst <- predict(c5DT1_c, mdTst)
table( pred = predTst, true=mdTst$y)
mean(predTst==mdTst$y)


#variable importance
C5imp(c5DT1_c)


```


2c. Develop a Random Forest Model
```{r}
library('randomForest')
set.seed(576)

rfModel200 = randomForest(y ~ ., data=mdTrn, ntree=200, importance=TRUE )
importance(rfModel200) %>% view()
varImpPlot(rfModel200)


rfModel100 = randomForest(y ~ ., data=mdTrn, ntree=100, importance=TRUE )
importance(rfModel100) %>% view()
varImpPlot(rfModel100)


rfModel50 = randomForest(y ~ ., data=mdTrn, ntree=50, importance=TRUE )
importance(rfModel50) %>% view()
varImpPlot(rfModel50)

#Choosing ntree = 100

#Classification Performance on Train Data
CTHRESH0.5 = 0.5

rfPredTrn0.5 <- predict(rfModel100,mdTrn, type="prob")
pred0.5 = ifelse(rfPredTrn0.5[, 'yes'] >= CTHRESH0.5, 'yes', 'no')
table( pred0.5 = pred0.5, true=mdTrn$y)
mean(pred0.5==mdTrn$y)


CTHRESH0.1 = 0.1

rfPredTrn0.1 <- predict(rfModel100, mdTrn, type="prob")
pred0.1 = ifelse(rfPredTrn0.1[, 'yes'] >= CTHRESH0.1, 'yes', 'no')
table( pred0.1 = pred0.1, true=mdTrn$y)
mean(pred0.1==mdTrn$y)


#Performance on Test Data
rfPredTst <-predict(rfModel100, mdTst, type="prob")
predTst = ifelse(rfPredTst[, 'yes'] >= CTHRESH0.5, 'yes', 'no')
table( predTst = predTst, true=mdTst$y)
mean(predTst==mdTst$y)


```




2d. Develop a GBM Model
```{r}
library(gbm)

#gbm looks for 0,1 values in the dependent variable -- obtained here using unclass()
gbm_M1_1000_0.025 <- gbm(formula=unclass(y)-1 ~., data=mdTrn,distribution = "bernoulli", n.trees=1000, shrinkage=0.025, interaction.depth = 4, bag.fraction=0.5, cv.folds = 5,  n.cores=NULL)

gbm_M1_500_0.025 <- gbm(formula=unclass(y)-1 ~., data=mdTrn,distribution = "bernoulli", n.trees=500, shrinkage=0.025, interaction.depth = 4, bag.fraction=0.5, cv.folds = 5,  n.cores=NULL)

gbm_M1_1000_0.125 <- gbm(formula=unclass(y)-1 ~., data=mdTrn,distribution = "bernoulli", n.trees=1000, shrinkage=0.125, interaction.depth = 4, bag.fraction=0.5, cv.folds = 5,  n.cores=NULL)

gbm_M1_500_0.125 <- gbm(formula=unclass(y)-1 ~., data=mdTrn,distribution = "bernoulli", n.trees=500, shrinkage=0.125, interaction.depth = 4, bag.fraction=0.5, cv.folds = 5,  n.cores=NULL)

#Determine best iteration
bestIter_1000_0.025 <- gbm.perf(gbm_M1_1000_0.025, method='cv')
bestIter_500_0.025 <- gbm.perf(gbm_M1_1000_0.025, method='cv')
bestIter_1000_0.125 <- gbm.perf(gbm_M1_1000_0.025, method='cv')
bestIter_500_0.025 <- gbm.perf(gbm_M1_1000_0.025, method='cv')

#Choosing n.trees = 1000 and shrinkage = 0.025

#Variable Importance
summary(gbm_M1_1000_0.025)

gbm_M1Trn <- predict(gbm_M1_1000_0.025, newdata = mdTrn, n.tree= bestIter_1000_0.025, type="response")
head(gbm_M1Trn)

#performance on Training Data
gbmPredTrn = predict(gbm_M1Trn, mdTrn, type='class')
table( gbmPredTrn = gbmPredTrn, true=mdTrn$y)
mean(gbmPredTrn==mdTrn$y)

#performance on Test Data
gbmPredTst = predict(gbm_M1Trn, mdTst)
table( gbmPredTst = gbmPredTst, true=mdTst$y)
mean(gbmPredTst==mdTst$y)

```


2e. Develop a Naive Bayes Model
```{r}
library(naivebayes)

# Develop a naive bayes model --------------------------------------------------
nbM1<-naive_bayes(y ~ ., data = mdTrn)
plot(nbM1)
summary(nbM1)

nbPred1Trn = predict(nbM1, mdTrn, type='prob')

#attempting various thresholds
THRESH1=0.2
table(pred=nbPred1Trn[, 2] > THRESH1, actual=mdTrn$y)
THRESH2=0.5
table(pred=nbPred1Trn[, 2] > THRESH2, actual=mdTrn$y)
THRESH3=0.8
table(pred=nbPred1Trn[, 2] > THRESH3, actual=mdTrn$y)

library('ROCR')
#ROC curve for nbM1 trn
pred_nbM1Trn = prediction(nbPred1Trn[,2], mdTrn$y)
perf_nbM1Trn <- performance(pred_nbM1Trn, "tpr","fpr")
plot(perf_nbM1Trn)
abline(a=0, b= 1)

#AUC
aucPerfnbM1 <- performance(pred_nbM1Trn , "auc")
aucPerfnbM1@y.values

#Accuracy 
accPerfnbM1 <- performance(pred_nbM1Trn, "acc")
plot(accPerfnbM1)

#optimal threshold for max overall accuracy
accPerfnbM1@x.values[[1]][which.max(accPerfnbM1@y.values[[1]])]


#optimal cost with different costs for fp and fn
costPerfnbM1 = performance(pred_nbM1Trn, "cost", cost.fp = 1, cost.fn = 3)
costPerfnbM1@x.values[[1]][which.min(costPerfnbM1@y.values[[1]])]

#Lift Curve for nbM1 trn
sc_nbM1Trn <- mdTrn %>%  select("y")
sc_nbM1Trn$score <- nbPred1Trn[, 2]
sc_nbM1Trn <- sc_nbM1Trn[order(sc_nbM1Trn$score, decreasing=TRUE),]
sc_nbM1Trn$cumResponse<-cumsum(sc_nbM1Trn$y == "yes")
plot(sc_nbM1Trn$cumResponse, type = "l", xlab='#cases', ylab='#default')
abline(0,max(sc_nbM1Trn$cumResponse)/nrow(sc_nbM1Trn), col="blue")  #diagonal line

nbPred1Tst = predict(nbM1, mdTst, type='prob')

#Lift Curve for nbM1 tst
predTstProb=predict(nbM1, mdTst, type='prob')
sc_nbM1Tst <- mdTst %>%  select("y")
sc_nbM1Tst$score <- nbPred1Tst[, 2]
sc_nbM1Tst<-sc_nbM1Tst[order(sc_nbM1Tst$score, decreasing=TRUE),]
sc_nbM1Tst$cumResponse<-cumsum(sc_nbM1Tst$y == "yes")

plot(sc_nbM1Tst$cumResponse, type = "l", xlab='#cases', ylab='#default')
abline(0,max(sc_nbM1Tst$cumResponse)/nrow(sc_nbM1Tst), col="blue")  #diagonal line

#performance on Training Data
nbPred1Trn = predict(nbM1, mdTrn)
table( nbPred1Trn = nbPred1Trn, true=mdTrn$y)
mean(nbPred1Trn==mdTrn$y)

#performance on Test Data
nbPred1Tst = predict(nbM1, mdTst)
table( nbPred1Tst = nbPred1Tst, true=mdTst$y)
mean(nbPred1Tst==mdTst$y)

# Develop a naive bayes model with KDE -----------------------------------------
nbM2<-naive_bayes(y ~ ., data = mdTrn, usekernel = T) 
plot(nbM2)
summary(nbM2)

nbPred2Trn = predict(nbM2, mdTrn, type='prob')

#ROC curve for nbM2 trn
pred_nbM2Trn = prediction(nbPred2Trn[,2], mdTrn$y)
perf_nbM2Trn <- performance(pred_nbM2Trn, "tpr","fpr")
plot(perf_nbM2Trn)
abline(a=0, b= 1)

#AUC
aucPerfnbM2 <- performance(pred_nbM2Trn , "auc")
aucPerfnbM2@y.values

#Accuracy 
accPerfnbM2 <- performance(pred_nbM2Trn, "acc")
plot(accPerfnbM2)

#optimal threshold for max overall accuracy
accPerfnbM2@x.values[[1]][which.max(accPerfnbM2@y.values[[1]])]


#optimal cost with different costs for fp and fn
costPerfnbM2 = performance(pred_nbM2Trn, "cost", cost.fp = 1, cost.fn = 3)
costPerfnbM2@x.values[[1]][which.min(costPerfnbM2@y.values[[1]])]

#Lift Curve for nbM2 trn
sc_nbM2Trn <- mdTrn %>%  select("y")
sc_nbM2Trn$score <- nbPred2Trn[, 2]
sc_nbM2Trn <- sc_nbM2Trn[order(sc_nbM2Trn$score, decreasing=TRUE),]
sc_nbM2Trn$cumResponse<-cumsum(sc_nbM2Trn$y == "yes")
plot(sc_nbM2Trn$cumResponse, type = "l", xlab='#cases', ylab='#default')
abline(0,max(sc_nbM2Trn$cumResponse)/nrow(sc_nbM2Trn), col="blue")  #diagonal line

nbPred2Tst = predict(nbM2, mdTst, type='prob')

#Lift Curve for nbM2 tst
predTstProb=predict(nbM2, mdTst, type='prob')
sc_nbM2Tst <- mdTst %>%  select("y")
sc_nbM2Tst$score <- nbPred1Tst[, 2]
sc_nbM2Tst<-sc_nbM1Tst[order(sc_nbM1Tst$score, decreasing=TRUE),]
sc_nbM2Tst$cumResponse<-cumsum(sc_nbM1Tst$y == "yes")

plot(sc_nbM2Tst$cumResponse, type = "l", xlab='#cases', ylab='#default')
abline(0,max(sc_nbM2Tst$cumResponse)/nrow(sc_nbM2Tst), col="blue")  #diagonal line

#performance on Training Data
nbPred2Trn = predict(nbM2, mdTrn)
table( nbPred2Trn = nbPred2Trn, true=mdTrn$y)
mean(nbPred2Trn==mdTrn$y)

#performance on Test Data
nbPred2Tst = predict(nbM2, mdTst)
table( nbPred2Tst = nbPred2Tst, true=mdTst$y)
mean(nbPred2Tst==mdTst$y)

```


2f. Compare Performance of Different Models Developed
```{r}

library('ROCR')

#rpart DT model -choosing model rpDT2_p
dtrPred <- predict(rpDT2_p, mdTst, type="prob")[,'yes']  
dtrROCPred <- prediction( dtrPred, mdTst$y, label.ordering = c("no", "yes") )
dtrPerfROC <- performance(dtrROCPred, "tpr", "fpr")
plot(dtrPerfROC, col='blue') 

#random forest model - choosing rfModel100
rfPred <-predict(rfModel100,mdTst, type="prob")[, 'yes']
rfROCPred <- prediction( rfPred, mdTst$y, label.ordering = c("no", "yes") )
rfPerfROC <- performance(rfROCPred, "tpr", "fpr")
plot(rfPerfROC, add=TRUE, col='green') 

#gbm model - choosing gbm_M1_1000_0.025
gbmPred <- predict(gbm_M1_1000_0.025, newdata = mdTst, n.tree= bestIter_1000_0.025, type="response")
gbmROCPred <- prediction( gbmPred, mdTst$y,label.ordering = c("no", "yes") )
gbmPerfROC <- performance(gbmROCPred, "tpr", "fpr")
plot(gbmPerfROC, add=TRUE, col='red')

#C5.0 - choosing c5DT1
c5Pred <- predict(c5DT1, mdTst, type='prob')[, "yes"]
c5ROCPred <- prediction( c5Pred, mdTst$y, label.ordering = c("no", "yes") )
c5PerfROC <- performance(c5ROCPred, "tpr", "fpr")
plot(c5PerfROC, add=TRUE, col='orange') 

#Add legend
legend('bottomright', c('c5', 'rpart', 'rf', 'gbm'), lty=1, col=c('orange', 'blue', 'green', 'red'))
abline(0,1)  #add the diagonal reference line


#Performance comparison
#rpart perfoamce
rpaccPerf <-performance(dtrROCPred, "acc")
plot(rpaccPerf,col='blue')
#c5 perfoamce
c5accPerf <-performance(c5ROCPred, "acc")
plot(c5accPerf,add=TRUE,col='Red')
#gbm perfoamce
gbmaccPerf <-performance(gbmROCPred , "acc")
plot(gbmaccPerf,add=TRUE,col='Orange')
#RF perfoamce
rfaccPerf <-performance(rfROCPred, "acc")
plot(rfaccPerf,add=TRUE,col='Green')
#Add legend
legend('bottomright', c('c5', 'rpart', 'rf', 'gbm'), lty=1, col=c('Red', 'blue', 'green', 'Orange'))
abline(0,1)  #add the diagonal reference line
